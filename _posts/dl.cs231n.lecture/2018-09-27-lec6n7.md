---
layout: post
title: Lecture 6,7 Training Neural Networks
category: Dl.cs231n.lecture
tags: ['DeepLearning', 'CS231n']
---

1. One time setup
  - Activation Functions
    - Sigmoid
      - Advantages: (-inf , inf) >> (0 , 1), good for saturating "firing rate" of neuron
      - Disadvantages: 
        - Saturated neuron's gradient is nearly 0: it stops gradient discent process
        - Sigmoid function is not zero-centered: results of activation function is always positive, which occurs the constraint of gradient discent
        - exp() is compute expansive
    - tanh
      - Advantages: similar with sigmoid, and zero-centered
      - Disadvantages: still saturated neuron kills gradients
    - ReLU(Rectified Linear Unit) = max(0, x)
      - Advantages:
        - Does not saturate in positive region
        - computationally efficient
        - converges faster than sigmoid/tanh
        -  more plausible in biological sight
      -  Disadvantages:
         - not zero-centered(Some people initialize ReLU neuron with slight positive biases to avoid this drawback)
         - gradient vanishes in negative region
     - Leaky ReLU = max(0.1 * x, x)
       - Advantages: Similar with ReLU, and gradients will not die at negative region
       - Similarly we have Parametric Rectifier(PReLU) f(x) = max(ax, x) (a is parameter)
     - ELU(Exponential Linear Unit) = max(a*(e^x - 1), x)
       - Advantages: Similar with ReLU, gradients will not die at negative region, closer to zero mean(centered) function, more robust to noise than Leaky ReLU
       - Disadvantages: Complex computation of exp()
     - Maxout = max(w * x + b, w' * x + b')
       - Advantages
         - Has Linear Regime
         - Does not Saturate
         - Generalized version of ReLU / Leaky ReLU
       - Disadvantages: it doubles the number of parameters
     - In Practice: Use ReLU, Try Leaky ReLU / ELU / Maxout, Don't expect much on tanh, Do not use sigmoid
![Activation Function](/public/img/cs231n/lec6.activation_function.png)
![Not-Zero-Centered Problem](/public/img/cs231n/lec6.not-zero-centered_problem_of_ReLU.png)

  - Data Preprocessing
    - There might be a constraint in all-positive or all-negative data: it is good to make data zero-centered
      - For each cases, we do subtract the total mean value or subtract the channel mean value for each channel
    - Another preprocessing might not be needed; since our data is all given pixels, we don't want to reduce its dimension or scale its description values.
![Data Preprocessing](/public/img/cs231n/lec6.data_preprocessing.png)
![Data Preprocessing 2](/public/img/cs231n/lec6.data_preprocessing_2.png)

  - Weight Initialization
    - If we initialize weight as 0, or other constant value, then every front-propagation and backpropagation computation value would be same for every node; Network would be needless
    - If we initialize weight as small random numbers, then as backpropagation between layers proceeds, every gradient goes 0 because of multiplication of small size weight value.
    - If we initialize weight as big random numbers, then neuron nodes would saturate as 1 or -1, gradients would be zero.
    - reasonable initialization by Xavier initialization {random number in (0,1)} / {root of {# of input node}}
      - In ReLU, we tend to use {root of {# of input node / 2}} instead of {root of {# of input node}}
![Small-scale Problem in Initialization](/public/img/cs231n/lec6.initialization_problem.png)
![Big-scale Problem in Initialization](/public/img/cs231n/lec6.initialization_problem_2.png)
![Xavier Initialization](/public/img/cs231n/lec6.xavier_initialization.png)
![Initialization in ReLU](/public/img/cs231n/lec6.initialization_in_ReLU.png)

  - Batch Normalization
    - Goal: keep the gaussian distribution of activation rate(that is, don't lead nodes to be dead)
    - Stretagy: Just make it happen! It's a matter of normalization.
    - Howto: for each dimension(in hidden layer, it might be each node) of mini batch, normalize the values for average and std. It works between FC layer and activation(tanh) function.
    - It gives robustness in range of learning rates/different initialization, and improves gradient flow
    - Practically, instead of using just normalized xhat, we also learn gamma and beta value. It would coordinate the result value while proceeding learning process
    - Practically, instead of solving each batch's mean/std, we might use single fixed empirical mean/std estimated from first training.
![Batch Normalization](/public/img/cs231n/lec6.batch_normalization.png)
![Batch Normalization 2](/public/img/cs231n/lec6.batch_normalization_2.png)
![Batch Normalization Algorithm](/public/img/cs231n/lec6.batch_normalization_algorithm.png)

  - Babysitting the Learning Process
    - Preprocessing > Choosing architecture > Varient Test > Train Data!
    - Sanity Check: for softmax of 10 classes, it is reasonable to check loss < 2.3 (= 10 * ln(1/10)) Also, we could check the increases of loss when cranking up the regularization.
    - Training Check: for small size of data, check for each epoch, cost goes nearly 0 and accuracy nearly 1. It should be done in small size of data(of course the result will be overfit, but anyway it's test).
    - Choose reasonable learning rate, otherwise loss would not going down or would explode to inf (1e-3 ~ 1e-5 is sufficient)

  - Hyperparameter Optimization
    - Cross Validation Strategy
      - For efficiency, first do coarse validation in few epochs. Then do fine validation in valid range of params, more epochs.
      - Do cross validation with log space!
      - Search hyperparameters with Random Search instead of Grid Search: it can find out the signals of distributions of hyperparameters more naturally.
    - Monitering the Hyperparameters with graph, or some indicator(ex: {gap between training acc and validation acc}, {weight updates}/{weight magnitudes})
![Cross Validation](/public/img/cs231n/lec6.coarse_cross_validation.png)
![Cross Validation 2](/public/img/cs231n/lec6.finer_cross_validation.png)
![Monitering Loss Curve](/public/img/cs231n/lec6.monitering_loss_curve.png)
![Monitering Loss Curve 2](/public/img/cs231n/lec6.monitering_loss_curve_2.png)
![Monitering Accuracy Curve](/public/img/cs231n/lec6.monitering_accuracy_curve.png)

2. Training Dynamics
3. Evaluation