---
layout: post
title: Lecture 4 Introduction to Neural Networks
category: Dl.cs231n.lecture
tags: ['DeepLearning', 'CS231n']
---

- Chain rule of Derivative: Get each partial derivative with computing a chain of derivative
  - add gate: distributing gradient
  - max gate: gradient router
  - mul gate: gradient switcher(scaler)
  - 1/x gate
  - exp gate or log gate
  - sigmoid gate
  - when calculate in vectorized form, the gradient comes as Jacobian Form, same matrix size with the shape of W.

![Simple example of chain rule](/public/img/cs231n/lec4.chain_rule.png)
![Example of chain rule in vector](/public/img/cs231n/lec4.chain_rule_vectorized_ver.png)
![Sigmoid Function](/public/img/cs231n/lec4.sigmoid_and_its_derivative.png)

- Backpropagation: After calculating Forward prediction(propagation) and calculating loss, calculate each partial derivatives by applying chain rule of derivative, then modipy parameters with these gradient. Iterate each Forward propagation and Backward propagation to minimize loss.

![Example of Forward and Backward Propagation](/public/img/cs231n/lec4.example_of_forward_backward_api.png)

- Neural Network: Stack multiple linear matrix multiplication layer(and also an activation layer), so that model gets non-linearity or various templates.

![How Neuron actually works](/public/img/cs231n/lec4.how_neuron_actually_works.png)
![Activation Functions](/public/img/cs231n/lec4.activation_functions.png)